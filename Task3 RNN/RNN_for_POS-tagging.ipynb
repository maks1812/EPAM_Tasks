{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RNN for POS-tagging problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this series we will try to use neural nets, in particular recurrent neural network to solve POS-tagging problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/chess1812/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/chess1812/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "sents = list(brown.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like last time, let's preprocess our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag types: {'CONJ', 'VERB', 'ADJ', 'NUM', 'X', 'DET', 'PRT', 'NOUN', '.', 'ADP', 'PRON', 'ADV'}\n"
     ]
    }
   ],
   "source": [
    "tags = set()\n",
    "\n",
    "for sent in sents:\n",
    "    for (word, tag) in sent: \n",
    "        tags.add(tag)   \n",
    "        \n",
    "NUM_CLASSES = len(tags)                \n",
    "print(f'tag types: {tags}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sents_train, sents_test = train_test_split(sents, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separete words from tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_train = []\n",
    "words_train = []\n",
    "for sent in sents_test:\n",
    "    tags_sent = []\n",
    "    words_sent = []\n",
    "    for (word, tag) in sent:\n",
    "        tags_sent.append(tag)\n",
    "        words_sent.append(word)\n",
    "    words_train.append(words_sent)\n",
    "    tags_train.append(tags_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_test = []\n",
    "words_test = []\n",
    "for sent in sents_test:\n",
    "    tags_sent = []\n",
    "    words_sent = []\n",
    "    for (word, tag) in sent:\n",
    "        tags_sent.append(tag)\n",
    "        words_sent.append(word)\n",
    "    words_test.append(words_sent)\n",
    "    tags_test.append(tags_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, we will use Keras framework. But Keras works with numbers, therefore we need to transform our words into numbers. To do that we will give each word in vocab an index and then we will replace words by their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Encoded data  example: \n",
      "\n",
      "X:  [275, 1066, 525] \n",
      "\n",
      "y:  [6, 1, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "word_tokenizer = Tokenizer()             \n",
    "word_tokenizer.fit_on_texts(words_train)         \n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(words_train)\n",
    "X_test =  word_tokenizer.texts_to_sequences(words_test)\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(tags_train)\n",
    "\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags_train)\n",
    "y_test = tag_tokenizer.texts_to_sequences(tags_test)\n",
    "\n",
    "# look at first encoded data point\n",
    "print(\" Encoded data  example: \\n\")\n",
    "print('X: ', X_train[0], '\\n')\n",
    "print('y: ', y_train[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the sentences in the corpus are not of the same length. Before we feed the input in the RNN model we need to fix the length of the sentences. We cannot dynamically allocate memory required to process each sentence in the corpus as they are of different lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'sentence len distibution')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = [len(x) for x in X_train]\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.hist(lens, bins =20)\n",
    "plt.title('sentence len distibution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's assume that sentence is no longer than 100.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first sequence: \n",
      "X:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0  275\n",
      " 1066  525] \n",
      "\n",
      "y:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 1 1]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQ_LENGTH = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "y_train = pad_sequences(y_train, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "y_test = pad_sequences(y_test, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "print('first sequence: ' )\n",
    "print('X: ',X_train[0], \"\\n\")\n",
    "print('y: ',y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = np.max(X_train) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one-hot encoding for output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader \n",
    "embeddings = gensim.downloader.load(\"fasttext-wiki-news-subwords-300\")\n",
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words which don't have embedding =  3082\n"
     ]
    }
   ],
   "source": [
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokenizer.word_index\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "not_in_embed = 0\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = embeddings[word]\n",
    "    except KeyError:\n",
    "        not_in_embed += 1\n",
    "        \n",
    "print(\"number of words which don't have embedding = \", not_in_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "def create_rnn_model(rnn_layer, embeddings = None, train_embedding = True):\n",
    "    rnn_model = Sequential()\n",
    "    rnn_model.add(layers.Embedding(input_dim = VOCABULARY_SIZE, \n",
    "                                   output_dim = EMBEDDING_SIZE, \n",
    "                                   input_length = MAX_SEQ_LENGTH, \n",
    "                                   trainable = train_embedding,   # True — update the embeddings  \n",
    "                                   weights = [embedding_weights] if embedding_weights is not None else None\n",
    "                                   ))\n",
    "\n",
    "    rnn_model.add(rnn_layer)\n",
    "    rnn_model.add(layers.TimeDistributed(layers.Dense(NUM_CLASSES + 1, activation='softmax')))\n",
    "\n",
    "    rnn_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
    "    return rnn_model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model is just simple RNN. But there are some ways for using embedding. We may train embeddings along with RNN layer or use pre-trained embeddings. So, as usual, let's try different  approaches and choose the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          6849000   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100, 64)           23360     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 13)           845       \n",
      "=================================================================\n",
      "Total params: 6,873,205\n",
      "Trainable params: 6,873,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_simple = create_rnn_model(rnn_layer = layers.SimpleRNN(64, return_sequences = True))\n",
    "rnn_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 25s 137ms/step - loss: 0.3497 - acc: 0.9320 - val_loss: 0.0686 - val_acc: 0.9835\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 24s 135ms/step - loss: 0.0452 - acc: 0.9881 - val_loss: 0.0261 - val_acc: 0.9930\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 26s 142ms/step - loss: 0.0233 - acc: 0.9930 - val_loss: 0.0177 - val_acc: 0.9947\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 24s 135ms/step - loss: 0.0171 - acc: 0.9945 - val_loss: 0.0140 - val_acc: 0.9957\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 24s 136ms/step - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0115 - val_acc: 0.9965\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 24s 136ms/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0097 - val_acc: 0.9970\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 24s 135ms/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0081 - val_acc: 0.9975\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 24s 134ms/step - loss: 0.0084 - acc: 0.9974 - val_loss: 0.0068 - val_acc: 0.9980\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 24s 133ms/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0058 - val_acc: 0.9984\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 24s 131ms/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0049 - val_acc: 0.9987\n"
     ]
    }
   ],
   "source": [
    "rnn_simple = rnn_simple.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          6849000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100, 64)           23360     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 100, 13)           845       \n",
      "=================================================================\n",
      "Total params: 6,873,205\n",
      "Trainable params: 24,205\n",
      "Non-trainable params: 6,849,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_with_pretrained_emb = create_rnn_model(layers.SimpleRNN(64, return_sequences = True), embeddings, False)\n",
    "rnn_with_pretrained_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 9s 52ms/step - loss: 0.5943 - acc: 0.8809 - val_loss: 0.3170 - val_acc: 0.9310\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 10s 54ms/step - loss: 0.2449 - acc: 0.9447 - val_loss: 0.1901 - val_acc: 0.9559\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 10s 53ms/step - loss: 0.1639 - acc: 0.9621 - val_loss: 0.1347 - val_acc: 0.9685\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 10s 53ms/step - loss: 0.1179 - acc: 0.9722 - val_loss: 0.1036 - val_acc: 0.9754\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 10s 55ms/step - loss: 0.0942 - acc: 0.9776 - val_loss: 0.0858 - val_acc: 0.9795\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 10s 53ms/step - loss: 0.0800 - acc: 0.9807 - val_loss: 0.0755 - val_acc: 0.9822\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 10s 54ms/step - loss: 0.0722 - acc: 0.9823 - val_loss: 0.0680 - val_acc: 0.9832\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 10s 54ms/step - loss: 0.0648 - acc: 0.9837 - val_loss: 0.0619 - val_acc: 0.9842\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 10s 54ms/step - loss: 0.0600 - acc: 0.9845 - val_loss: 0.0577 - val_acc: 0.9850\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 9s 51ms/step - loss: 0.0612 - acc: 0.9845 - val_loss: 0.0581 - val_acc: 0.9851\n"
     ]
    }
   ],
   "source": [
    "rnn_with_pretrained_emb = rnn_with_pretrained_emb.fit(\n",
    "                                                    X_train, \n",
    "                                                    y_train, \n",
    "                                                    batch_size=64, \n",
    "                                                    epochs=10, \n",
    "                                                    validation_data=(X_test, y_test\n",
    "                                                    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the results are worse. But what will be if we use pre-trained embeddings but allow the network to train them further. \n",
    "\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          6849000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 100, 64)           23360     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 100, 13)           845       \n",
      "=================================================================\n",
      "Total params: 6,873,205\n",
      "Trainable params: 6,873,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_with_emb = create_rnn_model(layers.SimpleRNN(64, return_sequences = True), embeddings)\n",
    "rnn_with_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 24s 134ms/step - loss: 0.3644 - acc: 0.9263 - val_loss: 0.0792 - val_acc: 0.9810\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 25s 136ms/step - loss: 0.0500 - acc: 0.9869 - val_loss: 0.0280 - val_acc: 0.9927\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 24s 134ms/step - loss: 0.0245 - acc: 0.9930 - val_loss: 0.0183 - val_acc: 0.9944\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 25s 138ms/step - loss: 0.0176 - acc: 0.9944 - val_loss: 0.0142 - val_acc: 0.9956\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 25s 137ms/step - loss: 0.0141 - acc: 0.9955 - val_loss: 0.0116 - val_acc: 0.9965\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 24s 134ms/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.0096 - val_acc: 0.9971\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 25s 139ms/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0081 - val_acc: 0.9977\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 25s 141ms/step - loss: 0.0084 - acc: 0.9974 - val_loss: 0.0069 - val_acc: 0.9980\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 25s 138ms/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0058 - val_acc: 0.9984\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 24s 135ms/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0049 - val_acc: 0.9987\n"
     ]
    }
   ],
   "source": [
    "rnn_with_emb = rnn_with_emb.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, score is more or less the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll build an LSTM model instead of an RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 300)          6849000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100, 64)           93440     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 100, 13)           845       \n",
      "=================================================================\n",
      "Total params: 6,943,285\n",
      "Trainable params: 6,943,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = create_rnn_model(layers.LSTM(64, return_sequences=True), embeddings)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 31s 173ms/step - loss: 0.5713 - acc: 0.8772 - val_loss: 0.2132 - val_acc: 0.9421\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 31s 174ms/step - loss: 0.1112 - acc: 0.9733 - val_loss: 0.0510 - val_acc: 0.9896\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 29s 164ms/step - loss: 0.0385 - acc: 0.9904 - val_loss: 0.0280 - val_acc: 0.9924\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 29s 163ms/step - loss: 0.0257 - acc: 0.9922 - val_loss: 0.0218 - val_acc: 0.9929\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 30s 168ms/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0187 - val_acc: 0.9937\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 30s 165ms/step - loss: 0.0184 - acc: 0.9937 - val_loss: 0.0164 - val_acc: 0.9945\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 30s 164ms/step - loss: 0.0163 - acc: 0.9943 - val_loss: 0.0145 - val_acc: 0.9951\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 31s 171ms/step - loss: 0.0147 - acc: 0.9950 - val_loss: 0.0130 - val_acc: 0.9957\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 31s 171ms/step - loss: 0.0134 - acc: 0.9955 - val_loss: 0.0119 - val_acc: 0.9961\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 30s 166ms/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0108 - val_acc: 0.9965\n"
     ]
    }
   ],
   "source": [
    "lstm_model = lstm_model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now build a GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 300)          6849000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100, 64)           70272     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 100, 13)           845       \n",
      "=================================================================\n",
      "Total params: 6,920,117\n",
      "Trainable params: 6,920,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = create_rnn_model(layers.GRU(64, return_sequences=True), embeddings)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 30s 168ms/step - loss: 0.4606 - acc: 0.9149 - val_loss: 0.0860 - val_acc: 0.9794\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 29s 163ms/step - loss: 0.0521 - acc: 0.9857 - val_loss: 0.0297 - val_acc: 0.9917\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 29s 162ms/step - loss: 0.0265 - acc: 0.9917 - val_loss: 0.0210 - val_acc: 0.9929\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 29s 163ms/step - loss: 0.0204 - acc: 0.9929 - val_loss: 0.0176 - val_acc: 0.9938\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 29s 161ms/step - loss: 0.0174 - acc: 0.9938 - val_loss: 0.0151 - val_acc: 0.9947\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 29s 163ms/step - loss: 0.0153 - acc: 0.9946 - val_loss: 0.0133 - val_acc: 0.9954\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 29s 163ms/step - loss: 0.0135 - acc: 0.9952 - val_loss: 0.0118 - val_acc: 0.9959\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 30s 164ms/step - loss: 0.0122 - acc: 0.9956 - val_loss: 0.0106 - val_acc: 0.9963\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 29s 163ms/step - loss: 0.0111 - acc: 0.9960 - val_loss: 0.0097 - val_acc: 0.9966\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 29s 164ms/step - loss: 0.0102 - acc: 0.9963 - val_loss: 0.0088 - val_acc: 0.9970\n"
     ]
    }
   ],
   "source": [
    "gru_model = gru_model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, neither LSTM nor GRU can't improve score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, why do we use our model only  in one direction?  That is a good point, in practice bidirectional model almost always works better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 300)          6849000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          186880    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 100, 13)           1677      \n",
      "=================================================================\n",
      "Total params: 7,037,557\n",
      "Trainable params: 7,037,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bidirect_model = create_rnn_model(layers.Bidirectional(layers.LSTM(64, return_sequences=True)), embeddings)\n",
    "bidirect_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 39s 218ms/step - loss: 0.4712 - acc: 0.8901 - val_loss: 0.1272 - val_acc: 0.9666\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 38s 209ms/step - loss: 0.0596 - acc: 0.9851 - val_loss: 0.0273 - val_acc: 0.9934\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 37s 208ms/step - loss: 0.0226 - acc: 0.9941 - val_loss: 0.0162 - val_acc: 0.9956\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 38s 210ms/step - loss: 0.0151 - acc: 0.9956 - val_loss: 0.0120 - val_acc: 0.9966\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 38s 213ms/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0096 - val_acc: 0.9973\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 38s 214ms/step - loss: 0.0096 - acc: 0.9972 - val_loss: 0.0077 - val_acc: 0.9979\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 39s 215ms/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0063 - val_acc: 0.9984\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 39s 218ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.0049 - val_acc: 0.9987\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 37s 204ms/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 39s 215ms/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.0031 - val_acc: 0.9993\n"
     ]
    }
   ],
   "source": [
    "bidirect_model = bidirect_model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it works better!\n",
    "\n",
    "To sum up, let's bring all results in one place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Most Frequent Class</td>\n",
       "      <td>95.72%</td>\n",
       "      <td>94.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Most Frequent Class with Random forest</td>\n",
       "      <td>97.36%</td>\n",
       "      <td>95.39%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hidden Markov Model</td>\n",
       "      <td>97.54%</td>\n",
       "      <td>96.02%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simple RNN</td>\n",
       "      <td>99.82%</td>\n",
       "      <td>99.87%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RNN with pre-trained embeddings</td>\n",
       "      <td>98.45%</td>\n",
       "      <td>98.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RNN with pre-trained embeddings + train</td>\n",
       "      <td>99.83%</td>\n",
       "      <td>99.87%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>99.59%</td>\n",
       "      <td>99.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GRU</td>\n",
       "      <td>99.63%</td>\n",
       "      <td>99.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bidirectional LSTM</td>\n",
       "      <td>99.90%</td>\n",
       "      <td>99.93%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     model train score test score\n",
       "0                      Most Frequent Class      95.72%     94.51%\n",
       "1   Most Frequent Class with Random forest      97.36%     95.39%\n",
       "2                      Hidden Markov Model      97.54%     96.02%\n",
       "3                               Simple RNN      99.82%     99.87%\n",
       "4          RNN with pre-trained embeddings      98.45%     98.51%\n",
       "5  RNN with pre-trained embeddings + train      99.83%     99.87%\n",
       "6                                     LSTM      99.59%     99.65%\n",
       "7                                      GRU      99.63%     99.70%\n",
       "8                       Bidirectional LSTM      99.90%     99.93%"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = {}\n",
    "dct['model'] = ['Most Frequent Class','Most Frequent Class with Random forest','Hidden Markov Model',\n",
    "                'Simple RNN','RNN with pre-trained embeddings','RNN with pre-trained embeddings + train',\n",
    "                 'LSTM', 'GRU', 'Bidirectional LSTM']\n",
    "\n",
    "dct['train score'] = ['95.72%', '97.36%', '97.54%', '99.82%', '98.45%', '99.83%' ,     '99.59%','99.63%' , '99.90%']\n",
    "dct['test score'] = ['94.51%', '95.39%', '96.02%', '99.87%', '98.51%', '99.87%', '99.65%', '99.70%', '99.93%']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=dct)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Recurrent neural networks  provide the opportunity solve the problems such as pos-tagging more effectively. Also it should not to be forgotten if you have opportunity to use bidirectional RNN you have to do that! \n",
    "But as usual, the best way to understand whether idea works or not is just try it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
