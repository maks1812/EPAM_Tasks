{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grapheme-to-Phoneme task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " amount of data 135154\n"
     ]
    }
   ],
   "source": [
    "with  open('/Users/chess1812/Documents/GitHub/EPAM_Tasks/Task4(grapheme-to-phoneme)/cmudict.txt') as file:\n",
    "    lines = []\n",
    "    for line in file:\n",
    "        lines.append(line.split())\n",
    "\n",
    "print(' amount of data', len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual  let's  see what we actually have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some examples:\n",
      "[\"'bout\", 'B', 'AW1', 'T']\n",
      "[\"'cause\", 'K', 'AH0', 'Z']\n",
      "[\"'course\", 'K', 'AO1', 'R', 'S']\n",
      "[\"'cuse\", 'K', 'Y', 'UW1', 'Z']\n",
      "[\"'em\", 'AH0', 'M']\n"
     ]
    }
   ],
   "source": [
    "print(\"some examples:\")\n",
    "for example in lines[0:5]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our task is to translate grapheme to phoneme. Sounds interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we have to divide our date and preprocess, nothing new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphemes = [ line[0] for line in lines]\n",
    "phonemes = [ line[1::] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "grapheme_train, grapheme_test, phoneme_train, phoneme_test = train_test_split(graphemes, phonemes, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [list(grapheme) for grapheme in grapheme_train]\n",
    "texts_test = [list(grapheme) for grapheme in grapheme_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean phoneme len =  6.385767605518012\n",
      "max phoneme len =  28\n",
      "min phoneme len =  1\n",
      "mean word len =  7.731202420296289\n",
      "max word len =  28\n",
      "min word len =  1\n"
     ]
    }
   ],
   "source": [
    "print('mean phoneme len = ', sum([len(phoneme) for phoneme in phoneme_train] ) / len(phoneme_train))\n",
    "print('max phoneme len = ', max([len(phoneme) for phoneme in phoneme_train] ) )\n",
    "print('min phoneme len = ', min([len(phoneme) for phoneme in phoneme_train] ) )\n",
    "\n",
    "print('mean word len = ', sum([len(word) for word in texts_train] ) / len(texts_train))\n",
    "print('max word len = ', max([len(word) for word in texts_train] ) )\n",
    "print('min word len = ', min([len(word) for word in texts_train] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phoneme distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AH0', 0.07314432405709931),\n",
       " ('N', 0.07102138134179249),\n",
       " ('S', 0.058496148062702126),\n",
       " ('L', 0.057889776917214246),\n",
       " ('T', 0.0567941891363009),\n",
       " ('R', 0.053790656477228256),\n",
       " ('K', 0.04998377860629905),\n",
       " ('D', 0.037784260613426164),\n",
       " ('IH0', 0.0348682719838507),\n",
       " ('M', 0.034366181226440355),\n",
       " ('Z', 0.03285862154201084),\n",
       " ('ER0', 0.027862174799678664),\n",
       " ('IY0', 0.025793303396708343),\n",
       " ('B', 0.02491915051393495),\n",
       " ('EH1', 0.024132541660658743),\n",
       " ('P', 0.023206892289945824),\n",
       " ('AA1', 0.019596988485385297),\n",
       " ('AE1', 0.019545491997445773),\n",
       " ('IH1', 0.01829927698930933),\n",
       " ('F', 0.01621238181556018),\n",
       " ('G', 0.01583388262920469),\n",
       " ('V', 0.012525233279090366),\n",
       " ('IY1', 0.011984520155725379),\n",
       " ('NG', 0.011537788122850022),\n",
       " ('EY1', 0.010926267328568192),\n",
       " ('HH', 0.010849022596658908),\n",
       " ('W', 0.010393278678394133),\n",
       " ('SH', 0.010211753558407317),\n",
       " ('OW1', 0.010080437514161534),\n",
       " ('OW0', 0.009582208993346653),\n",
       " ('AO1', 0.0095126887346283),\n",
       " ('AY1', 0.008034739530764001),\n",
       " ('AH1', 0.007921447257297052),\n",
       " ('UW1', 0.007795280861845222),\n",
       " ('JH', 0.007379446721733578),\n",
       " ('Y', 0.006124219828207716),\n",
       " ('CH', 0.005739283580859785),\n",
       " ('AA0', 0.005735421344264321),\n",
       " ('ER1', 0.005298988608976868),\n",
       " ('IH2', 0.0052140194038766555),\n",
       " ('EH2', 0.004561301419243207),\n",
       " ('EY2', 0.003974241456732651),\n",
       " ('AE2', 0.003958792510350794),\n",
       " ('AY2', 0.003945918388365914),\n",
       " ('AA2', 0.0038879848394339504),\n",
       " ('EH0', 0.0034155045625888316),\n",
       " ('TH', 0.003410354913794879),\n",
       " ('IY2', 0.0030344305518363647),\n",
       " ('OW2', 0.0028503306074525716),\n",
       " ('AW1', 0.002716439738809813),\n",
       " ('UW0', 0.0024241971697530227),\n",
       " ('AO2', 0.002201474859414588),\n",
       " ('AE0', 0.0019851896100685934),\n",
       " ('UH1', 0.001855160978021299),\n",
       " ('AO0', 0.0017238449337755165),\n",
       " ('AY0', 0.001386542937771644),\n",
       " ('UW2', 0.001364656930397347),\n",
       " ('AH2', 0.001306723381465384),\n",
       " ('EY0', 0.0011290604980740313),\n",
       " ('OY1', 0.0011071744906997344),\n",
       " ('AW2', 0.0007531361361155169),\n",
       " ('ER2', 0.0006977774115805302),\n",
       " ('DH', 0.0006720291676107689),\n",
       " ('ZH', 0.0006643046944198405),\n",
       " ('UH2', 0.0005535872453498672),\n",
       " ('AW0', 0.00044544462067686984),\n",
       " ('UH0', 0.00029610480565225454),\n",
       " ('OY2', 0.00023302160792633943),\n",
       " ('OY0', 0.00013517828084124662),\n",
       " ('#', 2.3173419572785135e-05),\n",
       " ('irish', 1.1586709786392568e-05),\n",
       " ('place,', 6.437060992440315e-06),\n",
       " ('name,', 5.149648793952252e-06),\n",
       " ('org,', 3.862236595464189e-06),\n",
       " ('abbrev', 2.574824396976126e-06),\n",
       " ('title,', 1.287412198488063e-06),\n",
       " ('german', 1.287412198488063e-06),\n",
       " ('old', 1.287412198488063e-06),\n",
       " ('name', 1.287412198488063e-06),\n",
       " ('dutch', 1.287412198488063e-06),\n",
       " ('finnish', 1.287412198488063e-06),\n",
       " ('danish', 1.287412198488063e-06),\n",
       " ('foreign', 1.287412198488063e-06),\n",
       " ('french', 1.287412198488063e-06)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "dct = defaultdict(int)\n",
    "for phoneme in phoneme_train:\n",
    "    for ph in phoneme:\n",
    "        dct[ph]+=1\n",
    "\n",
    "all_cnt = sum(len(phoneme) for phoneme in phoneme_train)\n",
    "value_cnt = []\n",
    "for phoneme_type, cnt in dct.items():\n",
    "    value_cnt.append((phoneme_type, cnt / all_cnt))\n",
    "    \n",
    "sorted(value_cnt, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add \"start\" and \"end\" symbol for feature algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(decoder_input):\n",
    "    bos = \"<BOS> \"\n",
    "    eos = \" <EOS>\"\n",
    "    final_target = decoder_input.copy()\n",
    "    final_target.append(eos)\n",
    "    final_target.insert(0, bos)\n",
    "    return final_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [tagger(word) for word in texts_train]\n",
    "texts_test = [tagger(word) for word in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_train = [tagger(phoneme) for phoneme in phoneme_train]\n",
    "phoneme_test = [tagger(phoneme) for phoneme in phoneme_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS> ', 's', 'e', 't', 't', 'l', 'e', 'm', 'e', 'n', 't', ' <EOS>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is well known, computer, in particular keras framework  works with numbers. So, we need to transform our date to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "phoneme_tokenizer = Tokenizer()             \n",
    "phoneme_tokenizer.fit_on_texts(phoneme_train)         \n",
    "\n",
    "y_train = phoneme_tokenizer.texts_to_sequences(phoneme_train)\n",
    "y_test = phoneme_tokenizer.texts_to_sequences(phoneme_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial  data  example: \n",
      " ['<BOS> ', 'S', 'EH1', 'T', 'AH0', 'L', 'M', 'AH0', 'N', 'T', ' <EOS>']\n",
      " Encoded data  example: \n",
      " [1, 5, 17, 7, 3, 6, 12, 3, 4, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "# look at first encoded data point\n",
    "print(\"initial  data  example: \\n\", phoneme_train[0])\n",
    "print(\" Encoded data  example: \\n\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial  data  example: \n",
      " ['<BOS> ', 's', 'e', 't', 't', 'l', 'e', 'm', 'e', 'n', 't', ' <EOS>']\n",
      " Encoded data  example: \n",
      " [1, 7, 3, 10, 10, 11, 3, 14, 3, 8, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "letter_tokenizer = Tokenizer()             \n",
    "letter_tokenizer.fit_on_texts(texts_train)         \n",
    "\n",
    "X_train = letter_tokenizer.texts_to_sequences(texts_train)\n",
    "X_test = letter_tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "# look at first encoded data point\n",
    "print(\"initial  data  example: \\n\", texts_train[0])\n",
    "print(\" Encoded data  example: \\n\", X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "NUM_EXAMPLES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different characters: 37\n",
      "number of different phonems: 87\n"
     ]
    }
   ],
   "source": [
    "VOCAB_INPUT_SIZE = len(letter_tokenizer.word_counts) + 1\n",
    "VOCAB_OUTPUT_SIZE = len(phoneme_tokenizer.word_counts) + 1\n",
    "VOCAB_SIZE = max(len(letter_tokenizer.word_counts), len(phoneme_tokenizer.word_counts)) + 1\n",
    "print('number of different characters:', VOCAB_INPUT_SIZE)\n",
    "print('number of different phonems:', VOCAB_OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input max len: 30\n",
      "output max len: 30\n"
     ]
    }
   ],
   "source": [
    "INPUT_MAX_LEN = max(len(x) for x in X_train)\n",
    "OUTPUT_MAX_LEN = max(len(y) for y in y_train)\n",
    "MAX_LEN =  max(INPUT_MAX_LEN, OUTPUT_MAX_LEN)\n",
    "print('input max len:', INPUT_MAX_LEN )\n",
    "print('output max len:', OUTPUT_MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "STEPS_PER_EPOCH = NUM_EXAMPLES //BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad each word to a maximum length. Why? Because we need to fix the maximum length for the inputs to recurrent encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_pad = pad_sequences(X_train, maxlen=INPUT_MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test, maxlen=INPUT_MAX_LEN, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pad = pad_sequences(y_train, maxlen=OUTPUT_MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "y_test_pad = pad_sequences(y_test, maxlen=OUTPUT_MAX_LEN, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using some neural nets, let's start with something simple. For our point of view KNN classifier may be a good baseline in this case. Firstly model finds some nearest neighbours and then use their translation with some coefficients to make a decision.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "\n",
    "class KNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, k = 5):\n",
    "        self.words = {}\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for x, label in zip(X,y):\n",
    "            self.words[x] = label\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = [None] * len(X)\n",
    "        for idx, x in enumerate(X):\n",
    "            if x in self.words:\n",
    "                y[idx] = self.words[x]\n",
    "                continue\n",
    "            closest_words = difflib.get_close_matches(x, list(self.words.keys()), n = self.k)\n",
    "            distances = [self.distance(word, x) for word in closest_words]\n",
    "            coeffs = [1 / dist  for dist in distances]\n",
    "            y[idx] = self.voting(coeffs, np.array([self.words[word] for word in closest_words]).T )\n",
    "        return y\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "    \n",
    "    def voting(self, coeffs, candidates):\n",
    "        result = []\n",
    "        for place in candidates:\n",
    "            dct = defaultdict(int)\n",
    "            for coeff,candidate in  zip(coeffs, place):\n",
    "                dct[candidate] += coeff\n",
    "            result.append( max([(coeff, candidate) for candidate, coeff in dct.items()])[1])\n",
    "        return result\n",
    "    \n",
    "    def distance(self, a, b):\n",
    "        \"Calculates the Levenshtein distance between a and b.\"\n",
    "        n, m = len(a), len(b)\n",
    "        if n > m:\n",
    "            # Make sure n <= m, to use O(min(n, m)) space\n",
    "            a, b = b, a\n",
    "            n, m = m, n\n",
    "\n",
    "        current_row = range(n + 1)  # Keep current and previous row, not entire matrix\n",
    "        for i in range(1, m + 1):\n",
    "            previous_row, current_row = current_row, [i] + [0] * n\n",
    "            for j in range(1, n + 1):\n",
    "                add, delete, change = previous_row[j] + 1, current_row[j - 1] + 1, previous_row[j - 1]\n",
    "                if a[j - 1] != b[i - 1]:\n",
    "                    change += 1\n",
    "                current_row[j] = min(add, delete, change)\n",
    "\n",
    "        return current_row[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time to train and test...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNClassifier(k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(grapheme_train, y_train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 9s, sys: 202 ms, total: 7min 9s\n",
      "Wall time: 7min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = knn.predict(grapheme_test[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  5, 21,  8, 15,  3,  6, 13,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32),\n",
       " array([ 1,  5, 21,  8, 15,  3,  6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pad[5], np.array(y_pred[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, somehow, it works. But how well ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare our models we will evaluate two metrics: first metric consider only exact match as right prediction. And second one just evaluate all phoneme matches as right prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_real, y_pred, return_percent = False):\n",
    "    right_answer_cnt = 0\n",
    "    for real, pred in zip(y_real, y_pred):\n",
    "        if list(real) == list(pred):\n",
    "            right_answer_cnt+=1\n",
    "    return right_answer_cnt / len(y_real) if return_percent else right_answer_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_count(y_real, y_pred, return_percent = False):\n",
    "    right_answer_cnt = 0\n",
    "    all_count = 0\n",
    "    for real, pred in zip(y_real, y_pred):\n",
    "         for real_el, pred_el in zip(real, pred):               \n",
    "            right_answer_cnt += (real_el == pred_el)\n",
    "         all_count += len(real)\n",
    "    return (right_answer_cnt / all_count) if return_percent else right_answer_cnt, all_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_distance(a, b):\n",
    "    \"Calculates the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "    if n > m:\n",
    "        # Make sure n <= m, to use O(min(n, m)) space\n",
    "        a, b = b, a\n",
    "        n, m = m, n\n",
    "\n",
    "    current_row = range(n + 1)  # Keep current and previous row, not entire matrix\n",
    "    for i in range(1, m + 1):\n",
    "        previous_row, current_row = current_row, [i] + [0] * n\n",
    "        for j in range(1, n + 1):\n",
    "            add, delete, change = previous_row[j] + 1, current_row[j - 1] + 1, previous_row[j - 1]\n",
    "            if a[j - 1] != b[i - 1]:\n",
    "                change += 1\n",
    "            current_row[j] = min(add, delete, change)\n",
    "\n",
    "    return current_row[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_metric(y_real, y_pred, return_mean_distance = False):\n",
    "    distance = np.array([lev_distance(real, pred) for real, pred in zip(y_real, y_pred)])\n",
    "    return np.mean(distance) if return_mean_distance else distance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.139"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test_pad[:1000], y_pred, return_percent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8969666666666667"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal_count(y_test_pad[:1000], y_pred, return_percent = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.578"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_metric(y_test_pad[:1000], y_pred, return_mean_distance = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so good. But wait, in example above we may see that our model could predict wrong ending. And at the same time  impact on our second metric a lot.  But ending  at some point is our fiction. So, probably we shouldnâ€™t  consider that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_result(inputs, beg_idx, end_idx):\n",
    "    start = 1 if inputs[0] == beg_idx  else  0\n",
    "    end = list(inputs).index(end_idx) if end_idx in list(inputs) else list(inputs).index(0)\n",
    "    return inputs[start: end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = phoneme_tokenizer.word_index['<bos> ']\n",
    "end_token = phoneme_tokenizer.word_index[' <eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.165"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [trim_result(y, start_token, end_token) for y in y_pred]\n",
    "y_real = [trim_result(y, start_token, end_token) for y in y_test_pad]\n",
    "accuracy(y_real[:1000], np.array(y_pred), return_percent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6447876447876448"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal_count(y_real[:1000], y_pred, return_percent = True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, now the result is better (by first metric). It's still  far away from completed. But as a baseline it is okay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq architecture is a common  decision  for such problems. Let's implement that! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train_pad), tf.convert_to_tensor(y_train_pad)))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(( tf.convert_to_tensor(X_test_pad), tf.convert_to_tensor(y_test_pad)))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 30]), TensorShape([64, 30]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        ##________ LSTM layer in Encoder ------- ##\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
    "        return output, h, c\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 30, 1024)\n",
      "Encoder h vecotr shape: (batch size, units) (64, 1024)\n",
      "Encoder c vector shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "## Test Encoder Stack\n",
    "encoder = Encoder(VOCAB_INPUT_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.attention_type = attention_type\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #Final Dense layer on which softmax will be applied\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # Define the fundamental cell for decoder recurrent structure\n",
    "        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "\n",
    "\n",
    "\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                                  None, self.batch_sz*[INPUT_MAX_LEN], self.attention_type)\n",
    "\n",
    "        # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "        self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "        # Define the decoder with respect to fundamental rnn cell\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "\n",
    "    def build_rnn_cell(self, batch_sz):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                      self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "        return rnn_cell\n",
    "\n",
    "    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
    "        # ------------- #\n",
    "        # typ: Which sort of attention (Bahdanau, Luong)\n",
    "        # dec_units: final dimension of attention outputs \n",
    "        # memory: encoder hidden states of shape (batch_size, INPUT_MAX_LEN, enc_units)\n",
    "        # memory_sequence_length: 1d array of shape (batch_size) with every element set to INPUT_MAX_LEN (for masking purpose)\n",
    "\n",
    "        if(attention_type=='bahdanau'):\n",
    "            return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "        else:\n",
    "            return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        x = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[OUTPUT_MAX_LEN-1])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (64, 29, 87)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder = Decoder(VOCAB_OUTPUT_SIZE, EMBEDDING_DIM, UNITS, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, OUTPUT_MAX_LEN))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defind optimisation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # real shape = (BATCH_SIZE, max_length_output)\n",
    "    # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "\n",
    "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "        real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "        # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "        # Create AttentionWrapperState as initial_state for decoder\n",
    "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_function(real, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.4716\n",
      "Time taken for 1 epoch 950.9030847549438 sec\n",
      "\n",
      "Epoch 2 Loss 0.1549\n",
      "Time taken for 1 epoch 1007.474799156189 sec\n",
      "\n",
      "Epoch 3 Loss 0.1264\n",
      "Time taken for 1 epoch 1023.8913309574127 sec\n",
      "\n",
      "Epoch 4 Loss 0.1093\n",
      "Time taken for 1 epoch 30184.14626097679 sec\n",
      "\n",
      "Epoch 5 Loss 0.1001\n",
      "Time taken for 1 epoch 1272.8709712028503 sec\n",
      "\n",
      "Epoch 6 Loss 0.0940\n",
      "Time taken for 1 epoch 1303.9953260421753 sec\n",
      "\n",
      "Epoch 7 Loss 0.0861\n",
      "Time taken for 1 epoch 1308.0938849449158 sec\n",
      "\n",
      "Epoch 8 Loss 0.0830\n",
      "Time taken for 1 epoch 1260.2351610660553 sec\n",
      "\n",
      "Epoch 9 Loss 0.0782\n",
      "Time taken for 1 epoch 1107.125169992447 sec\n",
      "\n",
      "Epoch 10 Loss 0.0762\n",
      "Time taken for 1 epoch 1288.6313879489899 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(STEPS_PER_EPOCH)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / STEPS_PER_EPOCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_step(inputs, preprocess = False):    \n",
    "    if preprocess:\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, UNITS)), tf.zeros((inference_batch_size,UNITS))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "    \n",
    "    start_token = phoneme_tokenizer.word_index['<bos> ']\n",
    "    start_tokens = tf.fill([inference_batch_size], start_token)\n",
    "    end_token = phoneme_tokenizer.word_index[' <eos>']\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "    # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "    # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "    # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "\n",
    "\n",
    "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id.numpy()\n",
    "    #return [trim_result(output, end_token) for output in outputs.sample_id.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict example: [ 8 20 12  6 54 29 15  2  2  2  2  2  2  2  2  2  2  2]\n",
      "real:  [ 1  8 45 12  6 52 15  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print('predict example:', translate_step(example_input_batch)[2])\n",
    "print('real: ',  example_target_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's transform that into readable format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(inputs, translate_algo = translate_step, preprocess = False ):\n",
    "    result = [trim_result(res, start_token, end_token ) for res in translate_algo(inputs, preprocess = preprocess) ]\n",
    "    return phoneme_tokenizer.sequences_to_texts(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict example: r ae1 m l ao2 w iy0\n",
      "real:  r ae2 m l aw1 iy0\n"
     ]
    }
   ],
   "source": [
    "predicted = translate(example_input_batch)\n",
    "real_indices = [trim_result(target, start_token, end_token) for target in example_target_batch.numpy()]\n",
    "real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "\n",
    "print('predict example:', predicted[2])\n",
    "print('real: ',  real[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. \n",
    "\n",
    "Now it's time to evaluate score. As in baseline example we will consider only  full convergence as right prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [02:20,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq with attention test accuracy: 0.4756584788398935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "right_cnt = 0\n",
    "\n",
    "for batch, (inp, targ) in tqdm(enumerate(test_dataset.take(STEPS_PER_EPOCH))):\n",
    "    y_pred = translate(inp)\n",
    "    real_indices = [trim_result(target, start_token, end_token) for target in targ.numpy()]\n",
    "    y_real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "    right_cnt += accuracy(y_real, y_pred)\n",
    "\n",
    "print('Seq2Seq with attention test accuracy:' ,right_cnt / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we consider accuracy just as number of right predicted phonemes divided by total number of phonemes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [02:21,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq with attention test soft accuracy: 0.8446156054154906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "right_cnt = 0\n",
    "all_cnt = 0\n",
    "for batch, (inp, targ) in tqdm(enumerate(test_dataset.take(STEPS_PER_EPOCH))):\n",
    "    y_pred = translate(inp)\n",
    "    real_indices = [trim_result(target, start_token, end_token) for target in targ.numpy()]\n",
    "    y_real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "    right, size = equal_count(y_real, y_pred)\n",
    "    right_cnt += right\n",
    "    all_cnt += size\n",
    "    \n",
    "print('Seq2Seq with attention test soft accuracy:' ,right_cnt / all_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [01:28,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq with attention test levenstain mean distance: 1.749408108907961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_res = []\n",
    "for batch, (inp, targ) in tqdm(enumerate(test_dataset.take(STEPS_PER_EPOCH))):\n",
    "    y_pred = translate(inp)\n",
    "    real_indices = [trim_result(target, start_token, end_token) for target in targ.numpy()]\n",
    "    y_real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "    all_res.append(lev_metric(y_real, y_pred))\n",
    "\n",
    "\n",
    "print('Seq2Seq with attention test levenstain mean distance:' ,\n",
    "      sum([sum(dist_lst) for dist_lst in all_res ]) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much better than our baseline. But for sure, there are many ways to improve. For instance we may use beam search instead of greedy algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_step(inputs, beam_width=3, preprocess = False):\n",
    "    \n",
    "    if preprocess:\n",
    "        pass\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, UNITS)), tf.zeros((inference_batch_size,UNITS))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], phoneme_tokenizer.word_index['<bos> '])\n",
    "    end_token = phoneme_tokenizer.word_index[' <eos>']\n",
    "\n",
    "    # From official documentation\n",
    "    # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
    "    # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
    "    # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
    "    # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
    "\n",
    "    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "    # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "    decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "    # Instantiate BeamSearchDecoder\n",
    "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "    # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
    "    # The final beam predictions are stored in outputs.predicted_id\n",
    "    # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
    "    # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
    "    # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
    "\n",
    "\n",
    "    # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "    # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "    # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
    "    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))[:,0] # taking only best version\n",
    "  #  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "\n",
    "    return final_outputs.numpy() #, beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict example: r aa0 m l aa1 w iy0\n",
      "real:  r ae2 m l aw1 iy0\n"
     ]
    }
   ],
   "source": [
    "predicted = translate(example_input_batch, translate_algo =  beam_step)\n",
    "real_indices = [trim_result(target, start_token, end_token) for target in example_target_batch.numpy()]\n",
    "real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "\n",
    "print('predict example:', predicted[2])\n",
    "print('real: ',  real[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [05:29,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam test accuracy:  0.48490677715300384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "right_cnt = 0\n",
    "\n",
    "for batch, (inp, targ) in tqdm(enumerate(test_dataset.take(STEPS_PER_EPOCH))):\n",
    "    y_pred = translate(inp, translate_algo =  beam_step)\n",
    "    real_indices = [trim_result(target, start_token, end_token) for target in targ.numpy()]\n",
    "    y_real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "    right_cnt += accuracy(y_real, y_pred)\n",
    "\n",
    "print('beam test accuracy: ', right_cnt / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_cnt = 0\n",
    "\n",
    "for batch, (inp, targ) in tqdm(enumerate(test_dataset.take(STEPS_PER_EPOCH))):\n",
    "    y_pred = translate(inp, translate_algo =  beam_step)\n",
    "    real_indices = [trim_result(target, start_token, end_token) for target in targ.numpy()]\n",
    "    y_real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "    right_cnt += accuracy(y_real, y_pred)\n",
    "\n",
    "print('beam test accuracy: ', right_cnt / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's evaluate soft accuracy like last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [05:10,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam test soft accuracy: 0.8490734257370626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "right_cnt = 0\n",
    "all_cnt = 0\n",
    "for batch, (inp, targ) in tqdm(enumerate(test_dataset.take(STEPS_PER_EPOCH))):\n",
    "    y_pred = translate(inp, translate_algo =  beam_step)\n",
    "    real_indices = [trim_result(target, start_token, end_token) for target in targ.numpy()]\n",
    "    y_real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "    right, size = equal_count(y_real, y_pred)\n",
    "    right_cnt += right\n",
    "    all_cnt += size\n",
    "    \n",
    "print('beam test soft accuracy:' ,right_cnt / all_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [03:13,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq with attention test levenstain mean distance: 1.6724622669428826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_res = []\n",
    "for batch, (inp, targ) in tqdm(enumerate(test_dataset.take(STEPS_PER_EPOCH))):\n",
    "    y_pred = translate(inp, translate_algo =  beam_step)\n",
    "    real_indices = [trim_result(target, start_token, end_token) for target in targ.numpy()]\n",
    "    y_real = phoneme_tokenizer.sequences_to_texts(real_indices )\n",
    "    all_res.append(lev_metric(y_real, y_pred))\n",
    "\n",
    "\n",
    "print('Seq2Seq with attention test levenstain mean distance:' ,\n",
    "      sum([sum(dist_lst) for dist_lst in all_res ]) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:** \n",
    "\n",
    "In this notebook we try to implement and use seq2seq arhitecture for solving grapheme to phoneme. Also there are a lot of things, that you could try and optimise such us attention mechanism, hyperparameters, search algorithm and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
