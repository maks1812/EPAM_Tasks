{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  POS-tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/chess1812/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/chess1812/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "sents = list(brown.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's  see what we actually have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag types: {'ADV', 'ADJ', 'CONJ', 'NOUN', '.', 'X', 'PRON', 'ADP', 'DET', 'PRT', 'NUM', 'VERB'}\n"
     ]
    }
   ],
   "source": [
    "tags = set()\n",
    "\n",
    "for sent in sents:\n",
    "    for (word, tag) in sent: \n",
    "        tags.add(tag)\n",
    "\n",
    "print(f'tag types: {tags}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sents_train, sents_test = train_test_split(sents, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some algorithms it may be better to transform tag type to numeric label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADV': 0,\n",
       " 'ADJ': 1,\n",
       " 'CONJ': 2,\n",
       " 'NOUN': 3,\n",
       " '.': 4,\n",
       " 'X': 5,\n",
       " 'PRON': 6,\n",
       " 'ADP': 7,\n",
       " 'DET': 8,\n",
       " 'PRT': 9,\n",
       " 'NUM': 10,\n",
       " 'VERB': 11}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_index = {}\n",
    "index_to_name = {}\n",
    "for idx, tag in enumerate(tags):\n",
    "    name_to_index[tag] = idx\n",
    "    index_to_name[idx] = tag\n",
    "    \n",
    "name_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "tags_train = []\n",
    "for sent in sents_train:\n",
    "    for (_, tag) in sent:\n",
    "        y_train.append(name_to_index[tag])\n",
    "        tags_train.append(tag)\n",
    "        \n",
    "y_train =  np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "tags_test = []\n",
    "for sent in sents_test:\n",
    "    for (_, tag) in sent:\n",
    "        y_test.append(name_to_index[tag])\n",
    "        tags_test.append(tag)\n",
    "        \n",
    "y_test =  np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "X_train = [word for word, tag in list(chain(*sents_train))]\n",
    "X_test = [word  for word, tag in  list(chain(*sents_test)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stats about corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag frequency: \n",
      "ADV: 4.84%\n",
      "ADJ: 7.20%\n",
      "CONJ: 3.30%\n",
      "NOUN: 23.75%\n",
      ".: 12.70%\n",
      "X: 0.11%\n",
      "PRON: 4.24%\n",
      "ADP: 12.48%\n",
      "DET: 11.79%\n",
      "PRT: 2.58%\n",
      "NUM: 1.29%\n",
      "VERB: 15.71%\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print('tag frequency: ')\n",
    "for unq, cnt in zip(unique, counts ):\n",
    "    print(f'{index_to_name[unq]}: { 100 * cnt / len(y_train):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 50595\n",
      "number of words = 929265\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "for sent in sents_train:\n",
    "    for (word, tag) in sent:\n",
    "        if word in vocab:\n",
    "            vocab[word].add(tag)\n",
    "        else:\n",
    "            vocab[word] = {tag}\n",
    "            \n",
    "print(f'vocab size = {len(vocab)}')\n",
    "print(f'number of words = {len(y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we shall see, the main problem in this task is to classificate some tricky words which may have different meaning. So, we have to determine its meaning from the context. Let's look at these words a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of tricky words in vocab 6.2832%\n",
      "percentage of tricky words in train corpora 43.8904%\n"
     ]
    }
   ],
   "source": [
    "tricky_words = set()\n",
    "for word in vocab:\n",
    "    if len(vocab[word]) > 1:\n",
    "        tricky_words.add(word)\n",
    "\n",
    "print(f'percentage of tricky words in vocab {100 * len(tricky_words) / len(vocab):.4f}%')\n",
    "\n",
    "amount_of_tricky_words  = len([word for word in X_train if word in  tricky_words])\n",
    "print(f'percentage of tricky words in train corpora {100 * amount_of_tricky_words / len(X_train):.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples of tricky words:\n",
      "pay: {'NOUN', 'VERB'}\n",
      "getting: {'NOUN', 'VERB'}\n",
      "process: {'NOUN', 'VERB'}\n",
      "cooler: {'ADJ', 'NOUN'}\n",
      "bronze: {'ADJ', 'NOUN'}\n"
     ]
    }
   ],
   "source": [
    "print('examples of tricky words:')\n",
    "examples_number = 5\n",
    "for word,_ in zip(tricky_words, range(examples_number)):\n",
    "    print(f'{word}: {vocab[word]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First idea that comes to mind is just remember is to use  **Most Frequent Class** model (assigning each token to the class\n",
    "it occurred in most often in the training set). For sure, in this way we will have a problem with words with different meaning but this at least model is a good baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MostFrequentClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, n_classes, unk_class = None):\n",
    "        self.vocab = {}\n",
    "        self.n_classes = n_classes\n",
    "        self.unk_class = unk_class\n",
    "        self.aprior_prob = np.zeros(self.n_classes)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):       \n",
    "        if self.unk_class is None:\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            for unq, count in zip(unique, counts):\n",
    "                self.aprior_prob[unq] = count / sum(counts)\n",
    "            self.unk_class = self.aprior_prob.argmax()\n",
    "        else:\n",
    "            self.aprior_prob[self.unk_class] = 1.0\n",
    "               \n",
    "        for word,tag in zip(X,y):\n",
    "            if word in self.vocab:\n",
    "                self.vocab[word][tag]+=1\n",
    "            else:\n",
    "                self.vocab[word] = [0] * self.n_classes\n",
    "                self.vocab[word][tag] = 1\n",
    "                \n",
    "        for word in self.vocab:\n",
    "            self.vocab[word] = np.array(self.vocab[word]) / sum(self.vocab[word])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [0]*len(X)\n",
    "        for idx,word in enumerate(X):\n",
    "            if word in self.vocab:\n",
    "                y_pred[idx] = self.vocab[word].argmax()\n",
    "            else:\n",
    "                y_pred[idx]= self.unk_class\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_pred = [0]*len(X)\n",
    "        for idx,word in enumerate(X):\n",
    "            if word in self.vocab:\n",
    "                y_pred[idx] = self.vocab[word]\n",
    "            else:\n",
    "                y_pred[idx]= self.aprior_prob\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MostFrequentClassifier(n_classes=12)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_clf = MostFrequentClassifier(n_classes = len(set(y_train)))\n",
    "baseline_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline score for train 95.722%\n",
      "baseline score for test 94.512%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = baseline_clf.predict(X_train)\n",
    "print(f'baseline score for train {100 * accuracy_score(y_train,y_pred):.3f}%')\n",
    "\n",
    "y_pred = baseline_clf.predict(X_test)\n",
    "print(f'baseline score for test {100 * accuracy_score(y_test,y_pred):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, it's not so bad, aready. But we will try to improve this score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's try to change  unk_class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MostFrequentClassifier(n_classes=12, unk_class=5)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_unk_clf = MostFrequentClassifier(n_classes = len(set(y_train)), unk_class = name_to_index['X'])\n",
    "baseline_unk_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline with default X as unk score for train 95.722\n",
      "baseline score default X as unk  for test 93.085\n"
     ]
    }
   ],
   "source": [
    "y_pred = baseline_unk_clf.predict(X_train)\n",
    "print(f'baseline with default X as unk score for train {100 * accuracy_score(y_train,y_pred):.3f}')\n",
    "\n",
    "y_pred = baseline_unk_clf.predict(X_test)\n",
    "print(f'baseline score default X as unk  for test {100 * accuracy_score(y_test,y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier.  the problem is that we do not use context and simply defined tag as most frequent class that we've seen. \n",
    "\n",
    "For instance word 'calls' has probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.33333333, 0.        ,\n",
       "       0.01754386, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.64912281])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_clf.vocab['calls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our model just says that I am not care about context, 'calls' is a verb in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_name[baseline_clf.predict(['calls'])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my thoughts was to use kNN when we face words with different meanings. We will fit kNN with words in which we are confident. And than if we face tricky word we will use our kNN model together with MFC classifier. For sure for kNN model we will use embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader \n",
    "embeddings = gensim.downloader.load(\"glove-wiki-gigaword-100\")   #\"fasttext-wiki-news-subwords-300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class MostFrequenWithkNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, n_classes, embeddings, alpha = 0.999,  beta = 0.5,  n_neighbors = 1, unk_class = None):\n",
    "        self.embeddings = embeddings\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta \n",
    "        self.neigh = KNeighborsClassifier(n_neighbors= n_neighbors )\n",
    "        self.vocab = {}\n",
    "        self.n_classes =  n_classes\n",
    "        self.aprior_prob = np.zeros(self.n_classes)\n",
    "        self.data_for_knn = []\n",
    "        self.labels_for_knn = []\n",
    "        self.unk_class = unk_class\n",
    "            \n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        if self.unk_class is None:\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            for unq, count in zip(unique, counts):\n",
    "                self.aprior_prob[unq] = count / sum(counts)\n",
    "            self.unk_class = self.aprior_prob.argmax()\n",
    "        else:\n",
    "            self.aprior_prob[self.unk_class] = 1.0\n",
    "            \n",
    "        for word,tag in zip(X,y):\n",
    "            if word in self.vocab:\n",
    "                self.vocab[word][tag]+=1\n",
    "            else:\n",
    "                self.vocab[word] = [0] * self.n_classes\n",
    "                self.vocab[word][tag] = 1\n",
    "                              \n",
    "        for word, label in zip(X,y):\n",
    "            self.vocab[word] = np.array(self.vocab[word]) / sum(self.vocab[word])\n",
    "            if np.max(self.vocab[word]  > self.alpha) and word in self.embeddings:\n",
    "                self.data_for_knn.append(self.embeddings[word])\n",
    "                self.labels_for_knn.append(label)\n",
    "                \n",
    "            self.neigh.fit(self.data_for_knn, self.labels_for_knn)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [0]*len(X)\n",
    "        self.unk_class = self.aprior_prob.argmax()\n",
    "        for idx,word in enumerate(X):\n",
    "            if word in self.vocab:\n",
    "                if np.max(self.vocab[word]  < self.alpha) and word in self.embeddings:\n",
    "                    knn_pred = neigh.predict_proba([self.embeddings[word]])[0]\n",
    "                    y_pred[idx] = (self.beta * self.vocab[word] + (1 - beta) * knn_pred).argmax()\n",
    "                else:\n",
    "                    y_pred[idx] = self.vocab[word].argmax()\n",
    "            else:\n",
    "                if word in self.embeddings:\n",
    "                    knn_pred = neigh.predict_proba([self.embeddings[word]])[0] \n",
    "                    y_pred[idx]= (self.beta * self.aprior_prob + (1 - beta) * knn_pred).argmax()\n",
    "                else:\n",
    "                     y_pred[idx]= self.unk_class                    \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_pred = [0]*len(X)\n",
    "        for idx,word in enumerate(X):\n",
    "            if word in self.vocab:\n",
    "                if np.max(self.vocab[word]  < self.alpha) and word in self.embeddings:\n",
    "                    knn_pred = neigh.predict_proba([self.embeddings[word]])[0]\n",
    "                    y_pred[idx] = (self.beta * self.vocab[word] + (1 - beta) * knn_pred)\n",
    "                else:\n",
    "                    y_pred[idx] = self.vocab[word]\n",
    "            else:\n",
    "                if word in self.embeddings:\n",
    "                    knn_pred = neigh.predict_proba([self.embeddings[word]])[0] \n",
    "                    y_pred[idx]= (self.beta * self.aprior_prob + (1 - beta) * knn_pred)\n",
    "                else:\n",
    "                     y_pred[idx]= self.aprior_prob                  \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_knn_clf =  MostFrequenWithkNNClassifier( len(set(y_train)),embeddings, alpha = 0.97, n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But unfortunately, Besides our idea do not use context too, it also works very slow due to the number of words, that we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3512c046fc80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreq_knn_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-a311a1ae0171>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_for_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_for_knn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_for_knn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    456\u001b[0m             self._tree = KDTree(X, self.leaf_size,\n\u001b[1;32m    457\u001b[0m                                 \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                 **self.effective_metric_params_)\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'brute'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "freq_knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some waiting hours I decided to stop that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = freq_knn_clf(X_train)\n",
    "#print(f'baseline with default X score for train {accuracy_score(y_train,y_pred)}')\n",
    "\n",
    "#y_pred = freq_knn_clf(X_test)\n",
    "#print(f'baseline score default X for test {accuracy_score(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest & LogisticRegression for context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second idea that comes to my mind is to build some classificator which having information about context will predict word tag. For simplicity we will build context features from the results of MFC algorithm. And than using these features we will fit some classifier such as random forest or logistics regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neigh_info(sents, clf, n_classes):\n",
    "    feature_vectors = []\n",
    "    for sent in sents:\n",
    "        words = [word for word, _ in sent]\n",
    "        predictions = clf.predict_proba(words)\n",
    "        if len(sent) == 1:\n",
    "            feature_vectors.append(np.hstack( (np.zeros(n_classes),np.zeros(n_classes))))\n",
    "            continue\n",
    "        feature_vectors.append( np.hstack( (np.zeros(n_classes), predictions[0])) )\n",
    "        for i in range(1, len(predictions) - 1):\n",
    "            feature_vectors.append(np.hstack( (predictions[i-1], predictions[i+1])) )\n",
    "        feature_vectors.append( np.hstack( (predictions[-1], np.zeros(n_classes))) )\n",
    "    return np.array(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = neigh_info(sents_train, baseline_clf, n_classes = len(set(y_train)))\n",
    "test_features = neigh_info(sents_test, baseline_clf, n_classes = len(set(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.   , 1.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.992, 0.   , 0.   ,\n",
       "       0.   , 0.   , 0.   , 0.   , 0.   , 0.008])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time to fit some classificators. Let's start with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf =  LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_clf.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression train score : 52.043%\n",
      "logistic regression test score 51.282%\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_clf.predict(train_features)\n",
    "print(f'logistic regression train score : {100 * accuracy_score(y_train,y_pred):.3f}%')\n",
    "\n",
    "y_pred = lr_clf.predict(test_features)\n",
    "print(f'logistic regression test score {100 * accuracy_score(y_test,y_pred):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest train score: 64.360%\n",
      "random forest test score:  59.630%\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_clf.predict(train_features)\n",
    "print(f'random forest train score: {100 * accuracy_score(y_train,y_pred):.3f}%')\n",
    "\n",
    "y_pred = rf_clf.predict(test_features)\n",
    "print(f'random forest test score:  {100 * accuracy_score(y_test,y_pred):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it's better than LogisticRegression. Also we may tune some paramentrs. For instance **max_features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(n_estimators=200,\n",
       "                                              random_state=42),\n",
       "             param_grid={'max_features': ['auto', 'sqrt', 'log2']})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'max_features' :['auto', 'sqrt', 'log2'] }\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators = 200)\n",
    "best_rf = GridSearchCV(rf, parameters)\n",
    "best_rf.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto'}\n"
     ]
    }
   ],
   "source": [
    " print(best_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sure we may tune other paraments too. But let's do it in anouther time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may think that we waste time because results are poor. But wait, we predict part of speach using only information about context but what will be if we combite two ideas together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class BlendingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, clf1, clf2, alpha = 0.5):\n",
    "        self.clf1 = clf1\n",
    "        self.clf2 = clf2\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    \n",
    "    def fit(self, X1,X2, y):  \n",
    "        self.clf1.fit(X1,y)\n",
    "        self.clf2.fit(X2,y)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X1,X2):\n",
    "        return np.argmax(self.alpha * np.array(self.clf1.predict_proba(X1)) +\n",
    "                (1 - self.alpha)* np.array(self.clf2.predict_proba(X2)), axis = 1)\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X1,X2):\n",
    "        return (self.alpha * np.array(self.clf1.predict_proba(X1)) +\n",
    "                (1 - self.alpha)* np.array(self.clf2.predict_proba(X2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlendingClassifier(clf1=MostFrequentClassifier(n_classes=12),\n",
       "                   clf2=RandomForestClassifier(n_estimators=200,\n",
       "                                               random_state=42))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = MostFrequentClassifier(n_classes = len(set(y_train)))\n",
    "clf2 = RandomForestClassifier(random_state=42, n_estimators = 200)\n",
    "\n",
    "mix_clf = BlendingClassifier( clf1,clf2  )\n",
    "mix_clf.fit(X_train, train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlendingClassifier train score: 97.357%\n",
      "BlendingClassifier test score:  95.389%\n"
     ]
    }
   ],
   "source": [
    "y_pred = mix_clf.predict(X_train,train_features)\n",
    "print(f'BlendingClassifier train score: {100 * accuracy_score(y_train,y_pred):.3f}%')\n",
    "\n",
    "y_pred = mix_clf.predict(X_test, test_features)\n",
    "print(f'BlendingClassifier test score:  {100 * accuracy_score(y_test,y_pred):.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improve our results approximately on $1\\%$ !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:** \n",
    "\n",
    "1) If we want to improve baseline results we should use context. And even such simple idea as we present may increase score. \n",
    "\n",
    "2) Also we may improve our BlendingClassifier by tuning parametr alpha. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will use HMM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to define transition probabilities, between tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_count = Counter(tags_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'NOUN': 220736,\n",
       "         '.': 118045,\n",
       "         'DET': 109545,\n",
       "         'VERB': 146006,\n",
       "         'ADP': 115998,\n",
       "         'CONJ': 30622,\n",
       "         'ADJ': 66949,\n",
       "         'PRON': 39400,\n",
       "         'ADV': 44968,\n",
       "         'PRT': 23976,\n",
       "         'NUM': 11963,\n",
       "         'X': 1057})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigrams =  Counter( [(tags_train[i], tags_train[i+1]) for i in range(0,len(tags_train)-2,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_starts = Counter([sent[0][1] for sent in sents_train] ) #number of times a tag occured at the start\n",
    "tag_ends = Counter([sent[-1][1]  for sent in sents_train] ) #number of times a tag occured at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'.': 44923,\n",
       "         'NOUN': 727,\n",
       "         'ADV': 18,\n",
       "         'DET': 14,\n",
       "         'VERB': 76,\n",
       "         'ADJ': 25,\n",
       "         'PRON': 4,\n",
       "         'NUM': 65,\n",
       "         'ADP': 8,\n",
       "         'CONJ': 2,\n",
       "         'PRT': 9,\n",
       "         'X': 1})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we need to define all words which occur from each tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(tags, words):\n",
    "    dct = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        dct[tag][word] += 1\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_words_count = pair_counts(tags_train,X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to put all in hmm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"hmm for POS-tagging\")\n",
    "     \n",
    "to_pass_states = []\n",
    "for tag, words_dict in tag_words_count.items():\n",
    "    total = float(sum(words_dict.values()))\n",
    "    distribution = {word: count/total for word, count in words_dict.items()}\n",
    "    tag_emissions = DiscreteDistribution(distribution)\n",
    "    tag_state = State(tag_emissions, name=tag)\n",
    "    to_pass_states.append(tag_state)\n",
    "    \n",
    "basic_model.add_states(to_pass_states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prob={}\n",
    "for tag in tags:\n",
    "    start_prob[tag]=tag_starts[tag]/ float(sum(tag_starts.values()))\n",
    "for tag_state in to_pass_states:\n",
    "    basic_model.add_transition(basic_model.start, tag_state, start_prob[tag_state.name])\n",
    "    \n",
    "end_prob={}\n",
    "for tag in tags:\n",
    "    end_prob[tag] = tag_ends[tag] /float(sum( tag_ends.values())) \n",
    "for tag_state in to_pass_states:\n",
    "    basic_model.add_transition(tag_state,basic_model.end, end_prob[tag_state.name])\n",
    "    \n",
    "transition_prob_pair={}\n",
    "for key in tag_bigrams.keys():\n",
    "    transition_prob_pair[key]=tag_bigrams.get(key) / tags_count[key[0]]\n",
    "    \n",
    "for tag_state in to_pass_states :\n",
    "    for next_tag_state in to_pass_states:\n",
    "        if (tag_state.name,next_tag_state.name) not in transition_prob_pair: # prob of this  transition = 0\n",
    "            transition_prob_pair[(tag_state.name,next_tag_state.name)] = 0.0\n",
    "        basic_model.add_transition(tag_state,next_tag_state,transition_prob_pair[(tag_state.name,next_tag_state.name)])\n",
    "basic_model.bake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we need to define what we should do with unknown words, and how do decode words into part of speach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence, vocab):   \n",
    "    return [w if w in vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(sequence, model):    \n",
    "    _, state_path = model.viterbi(replace_unknown(sequence, vocab))\n",
    "    return [state[1].name for state in state_path[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_predict(X, model):\n",
    "    y_pred = []\n",
    "    for observations in X:\n",
    "        most_likely_tags = simplify_decoding(observations, model)\n",
    "        y_pred.append(most_likely_tags)\n",
    "    return list(chain(*y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's look what happens from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_words = lambda sent: [word for (word,tag) in sent]\n",
    "observations_train = [only_words(sent) for sent in sents_train ]\n",
    "observations_test = [only_words(sent) for sent in sents_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy basic hmm model: 97.542%\n",
      "testing accuracy basic hmm model: 96.021%\n"
     ]
    }
   ],
   "source": [
    "hmm_training_acc = accuracy_score(tags_train, hmm_predict(observations_train, basic_model) )\n",
    "print(\"training accuracy basic hmm model: {:.3f}%\".format(100 * hmm_training_acc))\n",
    "hmm_testing_acc = accuracy_score(tags_test, hmm_predict(observations_test, basic_model) )\n",
    "print(\"testing accuracy basic hmm model: {:.3f}%\".format(100 * hmm_testing_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally hmm model archives best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:** \n",
    "\n",
    " In this notebook we try to solve pos-tagging problem. We have seen that the main difficulty is to classify polysemous words. Thus, we try to fit some models that will look at the context in order to solve this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
